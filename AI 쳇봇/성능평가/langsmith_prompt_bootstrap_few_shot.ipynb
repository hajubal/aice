{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bootstrap Few-shot Prompting with LangSmith\n",
        "## 작성자 : AISchool ( http://aischool.ai/%ec%98%a8%eb%9d%bc%ec%9d%b8-%ea%b0%95%ec%9d%98-%ec%b9%b4%ed%85%8c%ea%b3%a0%eb%a6%ac/ )\n",
        "## Reference : https://github.com/langchain-ai/langsmith-cookbook/blob/main/optimization/bootstrap-fewshot/bootstrap-few-shot.ipynb"
      ],
      "metadata": {
        "id": "pJ6i2qcdZ0Ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "프롬프트 엔지니어링은 매우 번거롭습니다. 그러나 LangSmith와 같은 도구를 사용하면 예제를 활용하여 프롬프트를 최적화할 수 있습니다. **어떤 예제가 가장 효과적일지 추측하는 대신, 검증된 평가 방식을 사용하여 파이프라인에 적합한 예제를 신중하게 선별하고 편집**할 수 있습니다. 주요 단계는 다음과 같습니다:\n",
        "\n",
        "\n",
        "1.   데이터셋 생성\n",
        "2.   개선할 지표 선택\n",
        "3.   초기 시스템 생성\n",
        "4.   업데이트 로직 결정 (few-shot examples vs. instruction teaching vs. other methods, how to format the examples, etc.)\n",
        "5.   학습!\n",
        "\n",
        "아래는 **few-shot examples을 사용하여 gpt-3.5-turbo 모델을 포함 작업(entailment task)에 부트스트래핑하는 예제**입니다. 이 예제는 SCONE 데이터셋에 대한 Christopher Potts의 예제( https://github.com/stanfordnlp/dspy/blob/main/examples/nli/scone/scone.ipynb )에서 영감을 받았습니다.\n",
        "\n",
        "task는 자연 언어 추론(Natural Language Inference)으로, LLM이 전제 / 근거 문장에서 논리적으로 결론을 도출할 수 있는지를 예측하는 것입니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qc-JzINKefuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 설치"
      ],
      "metadata": {
        "id": "Q-LtnSGBwVNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langsmith langchainhub langchain-openai tiktoken langchain_community"
      ],
      "metadata": {
        "id": "cnw7SkrwvUAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d25fb70-d032-45a4-bb71-257634ef1e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith\n",
            "  Downloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchainhub\n",
            "  Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.17-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain)\n",
            "  Downloading langchain_core-0.2.23-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.1)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
            "Collecting openai<2.0.0,>=1.32.0 (from langchain-openai)\n",
            "  Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.23->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.32.0->langchain-openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: types-requests, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, langchainhub, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.11 langchain-core-0.2.23 langchain-openai-0.1.17 langchain-text-splitters-0.2.2 langchain_community-0.2.10 langchainhub-0.1.20 langsmith-0.1.93 marshmallow-3.21.3 mypy-extensions-1.0.0 openai-1.37.0 orjson-3.10.6 tiktoken-0.7.0 types-requests-2.32.0.20240712 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from uuid import uuid4\n",
        "\n",
        "# Used by the agent in this tutorial\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"여러분의_OPENAI_API_KEY\"\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Bootstrap Few-shot Prompting- {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"여러분의_LANGCHAIN_API_KEY\""
      ],
      "metadata": {
        "id": "9_ovFmmWvT-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oBintcGifuET",
        "outputId": "83bbb111-9c3d-46a2-c7be-b158b2473431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2156ccdb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()"
      ],
      "metadata": {
        "id": "vK-Gyr7nf0G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_datasets = [\n",
        "    \"https://smith.langchain.com/public/1d065de2-56c1-496e-bc66-bdce308e6537/d\",  # train\n",
        "    \"https://smith.langchain.com/public/3205fa05-bd78-4eaf-924f-96df0f577b1f/d\",  # train2\n",
        "    \"https://smith.langchain.com/public/fdf16166-1edd-418f-b777-3af82034931d/d\",  # dev\n",
        "    \"https://smith.langchain.com/public/aee61506-3c60-4ca8-95c4-0314c9719ca8/d\",  # dev2\n",
        "    \"https://smith.langchain.com/public/8d40d210-f8e6-4def-a206-78c5080c5d53/d\",  # test\n",
        "]\n",
        "for ds in public_datasets:\n",
        "    client.clone_public_dataset(ds)"
      ],
      "metadata": {
        "id": "WK_1xm-NfyDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_name = \"scone-train2\"\n",
        "dev_name = \"scone-dev2\"\n",
        "test_name = \"scone-test-one-scoped\"\n",
        "full_test_name = \"scone-test\""
      ],
      "metadata": {
        "id": "4CB0vi5ybcrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = next(client.list_examples(dataset_name=train_name))\n",
        "print(\"inputs\", example.inputs)\n",
        "print(\"outputs\", example.outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxKKK5yGZ5rf",
        "outputId": "583dd469-22e9-4047-fe11-fb306d85c9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs {'context': 'A man who does not walk confidently dropping produce.', 'question': 'Can we logically conclude for sure that a man who does not walk confidently dropping kale?'}\n",
            "outputs {'answer': 'No', 'category': 'one_not_scoped'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs {'context': '자신 있게 걷지 않는 남자가 농산물을 떨어뜨린다.', 'question': '자신 있게 걷지 않는 남자가 케일을 떨어뜨린다고 논리적으로 확실히 결론지을 수 있습니까?'}\n",
        "# outputs {'answer': '아니요', 'category': '하나로 범위 지정되지 않음'}"
      ],
      "metadata": {
        "id": "YNjBVpqr1mXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 값을 검토해보면, 이러한 예제들은 꽤 어려운 문제라는 사실을 알 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c4ndS81Sf-nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluator\n"
      ],
      "metadata": {
        "id": "KLAxiCdogAeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리는 정답 분류 레이블을 가지고 있으므로, 정확히 일치하는 기준을 evaluator로 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "y06K7dny9Sf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "from langsmith.evaluation import run_evaluator\n",
        "\n",
        "\n",
        "@run_evaluator\n",
        "def exact_match(run, example):\n",
        "    # Evaluate the exact match correctness of the NLI result\n",
        "    try:\n",
        "        predicted = run.outputs[\"is_entailed\"]\n",
        "        expected = example.outputs[\"answer\"]\n",
        "        score = expected.lower() == predicted.lower()\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            expected = example.outputs[\"answer\"]\n",
        "            expected_bool = {\"no\": False, \"yes\": True}.get(expected.strip().lower())\n",
        "            score = run.outputs[\"output\"].is_entailed == expected_bool\n",
        "        except Exception as e2:\n",
        "            score = 0\n",
        "    return {\n",
        "        \"key\": \"exact_match\",\n",
        "        \"score\": int(score),\n",
        "    }"
      ],
      "metadata": {
        "id": "7_nsDjILf6eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# And we will create a placeholder in the template to add few-shot examples\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are given some context (a premise) and a question (a hypothesis). You must indicate with Yes/No answer whether we can logically conclude the hypothesis from the premise.\n",
        "\n",
        "---\n",
        "\n",
        "Follow the following format.\n",
        "\n",
        "Context: ${{context}}\n",
        "\n",
        "Question: ${{question}}\n",
        "\n",
        "Reasoning: Let's think step by step in order to ${{produce the answer}}. We ...\n",
        "\n",
        "Answer: Yes or No\n",
        "\n",
        "---{examples}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Reasoning: Let's think step by step in order to\"\"\"\n",
        ").partial(examples=\"\")\n",
        "\n",
        "\n",
        "def parse(pred: str):\n",
        "    fnd = \"\\nAnswer:\"\n",
        "    idx = pred.find(fnd)\n",
        "    answer = pred[idx + len(fnd) :].strip()\n",
        "    return {\"is_entailed\": answer, \"reasoning\": pred[:idx].strip()}\n",
        "\n",
        "\n",
        "chain = prompt | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser() | parse"
      ],
      "metadata": {
        "id": "bgvsxd5SfyBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = chain.invoke(example.inputs)"
      ],
      "metadata": {
        "id": "tqz2AcSEfx_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example.inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYjPhwSZHwpK",
        "outputId": "6bd9cbaa-e320-48b3-bb35-1b9f220348ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': 'A man who does not walk confidently dropping produce.',\n",
              " 'question': 'Can we logically conclude for sure that a man who does not walk confidently dropping kale?'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr1ElDOaHyso",
        "outputId": "3efc19e7-61bc-4ae3-f467-c360bbfb424a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is_entailed': 'No',\n",
              " 'reasoning': 'produce the answer. We know that dropping produce could include any type of produce, not specifically kale. So, we cannot logically conclude that a man who does not walk confidently drops kale.'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# {'is_entailed': '아니요',\n",
        "# 'reasoning': '답을 도출하십시오. 우리는 상품을 떨어뜨리는 것이 특정한 케일이 아니라\n",
        "# 다양한 종류의 상품을 포함할 수 있음을 알고 있습니다.\n",
        "# 따라서 자신감 있게 걷지 않는 남자가 케일을 떨어뜨린다고 논리적으로 결론을 내릴 수 없습니다.'}"
      ],
      "metadata": {
        "id": "Uqc_6Ie3IKe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Evaluation"
      ],
      "metadata": {
        "id": "7MfeL8v0EkEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[exact_match],\n",
        ")"
      ],
      "metadata": {
        "id": "EV6F_Cm9I-SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6rrgTvpnchCx",
        "outputId": "156f71c6-c10e-4d44-9587-8e3165c52fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'scone-dev2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.run_on_dataset(\n",
        "    dataset_name=dev_name,\n",
        "    llm_or_chain_factory=chain,\n",
        "    evaluation=eval_config,\n",
        "    project_metadata={\"optimizer\": None},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfpNLpp-MBR2",
        "outputId": "6dbe8461-83a3-4c58-b0a0-3b5dde820e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'upbeat-fire-7' at:\n",
            "https://smith.langchain.com/o/2da4358c-aaa8-5f93-b4af-fa2d78b96bd8/datasets/83064b47-7278-45d2-bba6-766f20d59705/compare?selectedSessions=dd30695c-d79c-41e1-b829-8ac3bb325d52\n",
            "\n",
            "View all tests for Dataset scone-dev2 at:\n",
            "https://smith.langchain.com/o/2da4358c-aaa8-5f93-b4af-fa2d78b96bd8/datasets/83064b47-7278-45d2-bba6-766f20d59705\n",
            "[------------------------------------------------->] 50/50"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "KOM-bmGAm6z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "약 50%의 정확도를 기록했습니다. 개선의 여지가 분명히 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2vXLS0jbNZjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✨ Optimize ✨"
      ],
      "metadata": {
        "id": "E6w-kPDdNcJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이는 **\"데이터를 사용하여 시스템을 업데이트한다\"**는 의미입니다. 현재 LangChain의 실행 가능 항목은 기본적으로 \"역방향\" 메서드를 지원하지 않지만(pytorch 방식처럼), **업데이트하고자 하는 주요 구성 요소(예: 프롬프트 또는 LLM)에 대한 업데이트/변경을 쉽게 정의**할 수 있습니다.\n",
        "\n",
        "예를 들어, 구성 요소별로 다음과 같이 적용할 수 있습니다:\n",
        "\n",
        "*   Few shot prompting: 프롬프트 템플릿에 추가 문자열 입력 또는 MessagesPlaceholder를 추가\n",
        "*   Updating the instructions: 프롬프트 템플릿을 직접 업데이트 (주로 시스템 프롬프트)\n",
        "*   LLM: 역방향 패스를 수행.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X7epjEHIOvJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리는 검색 공간을 제한하기 위해 **few-shot prompting에 집중**할 것입니다. 그런 다음 **유전/진화 알고리즘을 적용하여 다양한 few-shot 예제의 성능을 비교하고 제공된 지표에서 가장 큰 \"향상\"을 제공하는 예제를 선택**할 것입니다.\n",
        "\n",
        "먼저 few-shot 예제를 받아들이는 체인의 생성자를 만들어, 각 업데이트된 상태로 체인을 재생성할 수 있게 하겠습니다."
      ],
      "metadata": {
        "id": "ZaUC6OdCQfQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will define how we want our few-shot examples to be formatted\n",
        "import random\n",
        "from typing import List, Optional\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "\n",
        "def format_example(example: dict):\n",
        "    inputs = example[\"input\"]\n",
        "    outputs = example[\"output\"]\n",
        "    return f\"\"\"\n",
        "\n",
        "Context: {inputs['context']}\n",
        "\n",
        "Question: {inputs['question']}\n",
        "\n",
        "Reasoning: {outputs['reasoning']}\n",
        "\n",
        "Answer: {outputs['is_entailed']}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def format_few_shot(input_: dict, examples: Optional[List[dict]] = None):\n",
        "    if examples:\n",
        "        # TODO: make this configurable / bound to the prompt template\n",
        "        input_[\"examples\"] = (\n",
        "            \"--\".join(format_example(e) for i, e in enumerate(examples)) + \"--\"\n",
        "        )\n",
        "    return input_\n",
        "\n",
        "\n",
        "def create_chain(examples: Optional[List] = None, llm=None):\n",
        "    llm = llm or ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "    chain = (\n",
        "        RunnableLambda(format_few_shot).bind(examples=examples)\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "        | parse\n",
        "    ).with_config(tags=[\"to_train\"])\n",
        "    return chain"
      ],
      "metadata": {
        "id": "kpgDJzWKEsUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "9GY-8DOAR6Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음으로, 훈련 유틸리티를 정의하겠습니다."
      ],
      "metadata": {
        "id": "I6sb6-AtSBDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tracers.context import collect_runs"
      ],
      "metadata": {
        "id": "KciidjWyIs1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(\n",
        "    construct_chain,\n",
        "    train_examples,\n",
        "    eval_config,\n",
        "    examples=None,\n",
        "    bootstrap_k: int = 8,\n",
        "):\n",
        "    collected = examples.copy() if examples else []\n",
        "    random.shuffle(train_examples)\n",
        "    train_examples = train_examples.copy()\n",
        "    # TODO: Batching to speed it up\n",
        "    while train_examples:\n",
        "        if len(collected) >= bootstrap_k:\n",
        "            break\n",
        "        train_batch = [\n",
        "            train_examples.pop() for _ in range(bootstrap_k - len(collected))\n",
        "        ]\n",
        "        chain = construct_chain([e for e in collected if e[\"id\"] != example.id])\n",
        "        with collect_runs() as cb:\n",
        "            chain.batch([e.inputs for e in train_batch])\n",
        "        evaluator = eval_config.custom_evaluators[0]\n",
        "        for run, example in zip(cb.traced_runs, train_batch):\n",
        "            metric = evaluator.evaluate_run(run, example)\n",
        "            score = metric.score\n",
        "            # Check if success\n",
        "            if score:\n",
        "                collected.append(\n",
        "                    {\n",
        "                        \"input\": example.inputs,\n",
        "                        \"output\": run.outputs,\n",
        "                        \"id\": example.id,\n",
        "                    }\n",
        "                )\n",
        "    return collected"
      ],
      "metadata": {
        "id": "X3YtM7_QHoMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(eval_dataset, chain, eval_config, step_n) -> float:\n",
        "    \"\"\"Compute the metrics on the validation dataset.\"\"\"\n",
        "    dev_results = client.run_on_dataset(\n",
        "        dataset_name=eval_dataset,\n",
        "        llm_or_chain_factory=chain,\n",
        "        evaluation=eval_config,\n",
        "        verbose=True,\n",
        "        concurrency_level=1,\n",
        "        project_metadata={\n",
        "            \"step\": step_n,\n",
        "        },\n",
        "    )\n",
        "    df = dev_results.to_dataframe()\n",
        "    feedback_key = [c for c in df.columns if c.startswith(\"feedback.\")][0]\n",
        "    # Assume single metric rn ha\n",
        "    return df[feedback_key].mean()"
      ],
      "metadata": {
        "id": "RfQd2b6nHpel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    chain_constructor,\n",
        "    train_dataset,\n",
        "    eval_dataset,\n",
        "    eval_config,\n",
        "    steps: int = 5,\n",
        "    k: int = 8,\n",
        "    bootstrap_k: int = 8,\n",
        "):\n",
        "    \"\"\"Run the full training loop\"\"\"\n",
        "    best_score = eval(eval_dataset, chain_constructor(), eval_config, 0)\n",
        "    best_step = 0\n",
        "    scores = [(best_score, [])]\n",
        "    train_examples = list(client.list_examples(dataset_name=train_dataset))\n",
        "    for step_number in range(steps):\n",
        "        collected = step(\n",
        "            chain_constructor, train_examples, eval_config, bootstrap_k=bootstrap_k\n",
        "        )\n",
        "        if len(collected) < k:\n",
        "            # TODO: probably want some diversity of labels here lol\n",
        "            to_sample = min(k - len(collected), len(train_examples))\n",
        "            collected += random.sample(train_examples, to_sample)\n",
        "        selected_examples = collected\n",
        "        updated_chain = chain_constructor(examples=selected_examples)\n",
        "        updated_score = eval(eval_dataset, updated_chain, eval_config, step_number + 1)\n",
        "        scores.append((updated_score, selected_examples))\n",
        "\n",
        "        if updated_score > best_score:\n",
        "            print(\n",
        "                f\"New best score {updated_score} > {best_score}. Updating selected examples.\"\n",
        "            )\n",
        "            best_score = updated_score\n",
        "            best_step = step_number + 1\n",
        "        else:\n",
        "            print(\"Underperformed. Continuing\")\n",
        "    print(\"Best overall score: \", best_score)\n",
        "    print(\"Best step: \", best_step)\n",
        "    return sorted(scores, key=lambda x: x[0], reverse=True)"
      ],
      "metadata": {
        "id": "N64pp4V8EsR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n"
      ],
      "metadata": {
        "id": "-m0RXeoUUj7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 마침내 훈련 루프를 실행할 수 있습니다!"
      ],
      "metadata": {
        "id": "6m7DrxEEUvMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "# We will train with gpt-4o\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "all_scores = train(\n",
        "    functools.partial(create_chain, llm=llm),\n",
        "    train_name,\n",
        "    dev_name,\n",
        "    eval_config,\n",
        "    #steps=10,\n",
        "    steps=1,\n",
        ")"
      ],
      "metadata": {
        "id": "Uiv-5EZNEfmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICQzjqVVEfjf",
        "outputId": "2eb7ec03-dcc8-4cbe-836f-01786ccb4a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9375,\n",
              "  [{'input': {'context': 'The three children are not in the classroom, but they are holding sunflowers.',\n",
              "     'question': 'Can we logically conclude for sure that the three children are not in the classroom, but they are holding plants?'},\n",
              "    'output': {'is_entailed': 'Yes',\n",
              "     'reasoning': \"Reasoning: Let's think step by step in order to produce the answer. We know from the context that the three children are holding sunflowers. Sunflowers are a type of plant. Therefore, if the children are holding sunflowers, they are indeed holding plants.\"},\n",
              "    'id': UUID('8cef87d4-d3d2-4367-8689-824855c7cb0c')},\n",
              "   {'input': {'context': 'The dog liked sandwich, but not from the store that is not far away.',\n",
              "     'question': 'Can we logically conclude for sure that the dog liked food, but not from the store that is not far away?'},\n",
              "    'output': {'is_entailed': 'Yes',\n",
              "     'reasoning': 'Reasoning: Let\\'s think step by step in order to produce the answer. We know from the context that the dog liked the sandwich but did not like the sandwich from the store that is not far away. The term \"food\" is broader than \"sandwich\" and it is not explicitly stated whether the dog liked other types of food from or not from the store. However, since a sandwich is a type of food, and the context does specify the dog\\'s preferences regarding the sandwich, we can infer that the dog liked a type of food but not from the store that is not far away.'},\n",
              "    'id': UUID('517c33d6-8a4b-42ad-a0dd-f4ca7494d35f')},\n",
              "   {'input': {'context': 'The people were not trying to keep their voices down so they woke a mother who is not indoors.',\n",
              "     'question': 'Can we logically conclude for sure that the people were not trying to keep their voices down so they woke a woman who is not indoors?'},\n",
              "    'output': {'is_entailed': 'Yes',\n",
              "     'reasoning': 'Reasoning: Let\\'s think step by step in order to produce the answer. We need to examine the context and the question carefully. \\n\\n1. The context states that \"the people were not trying to keep their voices down so they woke a mother who is not indoors.\"\\n2. The question asks if we can logically conclude that \"the people were not trying to keep their voices down so they woke a woman who is not indoors.\"\\n\\nWe need to check if the terms \"mother\" and \"woman\" can be assumed to be equivalent in this context. Since all mothers are women, we can safely make this assumption.\\n\\nNext, we need to ensure that the rest of the context aligns with the question. The context mentions that the mother was woken up and that she was not indoors, which matches the situation described in the question.'},\n",
              "    'id': UUID('43e5cb35-e7cf-4fff-b441-fd7917d6cc9c')},\n",
              "   {'input': {'context': 'There is not a single person walking in the city.',\n",
              "     'question': 'Can we logically conclude for sure that there is not a single musher walking in the city?'},\n",
              "    'output': {'is_entailed': 'Yes',\n",
              "     'reasoning': \"Reasoning: Let's think step by step in order to produce the answer. We know from the context that there is not a single person walking in the city. A musher is a type of person, specifically someone who drives a dog sled. Since the context states there are no people walking in the city, this would include all types of people, including mushers.\"},\n",
              "    'id': UUID('2e61737e-452a-46b1-abdb-eff4a5a1c4d5')},\n",
              "   {'input': {'context': 'The girl who is not weak is wearing some jewelry, but not a dress.',\n",
              "     'question': 'Can we logically conclude for sure that the girl who is not weak is wearing some rings, but not a dress?'},\n",
              "    'output': {'is_entailed': 'No',\n",
              "     'reasoning': \"Reasoning: Let's think step by step in order to produce the answer. We know from the context that the girl who is not weak is wearing some jewelry but not a dress. However, the context does not specify what type of jewelry she is wearing. Jewelry could include rings, necklaces, bracelets, earrings, etc. Therefore, we cannot logically conclude for sure that the girl is wearing rings specifically.\"},\n",
              "    'id': UUID('bde6abce-9288-4a3b-9a4e-4a7642e9cdc7')},\n",
              "   {'input': {'context': 'The girl who is not here is not wearing any jewelry at all.',\n",
              "     'question': 'Can we logically conclude for sure that the girl who is not here is not wearing any ringlets at all?'},\n",
              "    'output': {'is_entailed': 'Yes',\n",
              "     'reasoning': 'Reasoning: Let\\'s think step by step in order to produce the answer. We start by understanding the context: \"The girl who is not here is not wearing any jewelry at all.\" This statement means that any form of jewelry, including ringlets, is not being worn by the girl who is not present. Since ringlets are a type of jewelry, if she is not wearing any jewelry, she is also not wearing ringlets.'},\n",
              "    'id': UUID('d2050091-b628-40bb-a3de-22060bdba537')},\n",
              "   {'input': {'context': 'The person who is not allergic likes raspberries but prefers not to eat vegetables.',\n",
              "     'question': 'Can we logically conclude for sure that the person who is not allergic likes fruit but prefers not to eat vegetables?'},\n",
              "    'output': {'is_entailed': 'Yes',\n",
              "     'reasoning': \"Reasoning: Let's think step by step in order to produce the answer. We know from the context that the person who is not allergic likes raspberries and prefers not to eat vegetables. Raspberries are a type of fruit. Therefore, it is logical to conclude that the person likes at least one type of fruit (raspberries). Since the context also states that the person prefers not to eat vegetables, it aligns with the question's statement about their preference regarding vegetables.\"},\n",
              "    'id': UUID('82109203-1216-494a-9ed4-8b396d2f3c56')},\n",
              "   {'input': {'context': 'A man is holding something in his hands.',\n",
              "     'question': 'Can we logically conclude for sure that a man is holding stamps in his hands?'},\n",
              "    'output': {'is_entailed': 'No',\n",
              "     'reasoning': 'Reasoning: Let\\'s think step by step in order to produce the answer. We know from the context that a man is holding something in his hands. However, the context does not specify what that \"something\" is. It could be anything, not necessarily stamps. Therefore, we cannot logically conclude for sure that the man is holding stamps in his hands.'},\n",
              "    'id': UUID('9f11d197-e72a-4307-b39f-2d5e89f5dc5e')}]),\n",
              " (0.8260869565217391, [])]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare on held-out set"
      ],
      "metadata": {
        "id": "739aVF0gXDM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단일 벤치마크를 기준으로 파이프라인을 선택하면 해당 벤치마크에서 과적합(overfit)하기 쉽습니다.\n",
        "\n",
        "선택된 예제가 신뢰할 수 있게 더 나은지 확인하기 위해 **보지 않은 테스트 세트(unseen test set)에서 모델을 비교**해봅시다.\n"
      ],
      "metadata": {
        "id": "hQuy1nWCX0fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_score, best_examples = all_scores[0]"
      ],
      "metadata": {
        "id": "Q3hkFZG4gEvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgiVJ6OYXIP8",
        "outputId": "a3af64d7-cf7d-45b3-fda0-b296d85b3856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9375"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDbPjNqcXJMS",
        "outputId": "461d6438-472f-49d8-fcad-59686ac6b52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': {'context': 'The three children are not in the classroom, but they are holding sunflowers.',\n",
              "   'question': 'Can we logically conclude for sure that the three children are not in the classroom, but they are holding plants?'},\n",
              "  'output': {'is_entailed': 'Yes',\n",
              "   'reasoning': \"Reasoning: Let's think step by step in order to produce the answer. We know from the context that the three children are holding sunflowers. Sunflowers are a type of plant. Therefore, if the children are holding sunflowers, they are indeed holding plants.\"},\n",
              "  'id': UUID('8cef87d4-d3d2-4367-8689-824855c7cb0c')},\n",
              " {'input': {'context': 'The dog liked sandwich, but not from the store that is not far away.',\n",
              "   'question': 'Can we logically conclude for sure that the dog liked food, but not from the store that is not far away?'},\n",
              "  'output': {'is_entailed': 'Yes',\n",
              "   'reasoning': 'Reasoning: Let\\'s think step by step in order to produce the answer. We know from the context that the dog liked the sandwich but did not like the sandwich from the store that is not far away. The term \"food\" is broader than \"sandwich\" and it is not explicitly stated whether the dog liked other types of food from or not from the store. However, since a sandwich is a type of food, and the context does specify the dog\\'s preferences regarding the sandwich, we can infer that the dog liked a type of food but not from the store that is not far away.'},\n",
              "  'id': UUID('517c33d6-8a4b-42ad-a0dd-f4ca7494d35f')},\n",
              " {'input': {'context': 'The people were not trying to keep their voices down so they woke a mother who is not indoors.',\n",
              "   'question': 'Can we logically conclude for sure that the people were not trying to keep their voices down so they woke a woman who is not indoors?'},\n",
              "  'output': {'is_entailed': 'Yes',\n",
              "   'reasoning': 'Reasoning: Let\\'s think step by step in order to produce the answer. We need to examine the context and the question carefully. \\n\\n1. The context states that \"the people were not trying to keep their voices down so they woke a mother who is not indoors.\"\\n2. The question asks if we can logically conclude that \"the people were not trying to keep their voices down so they woke a woman who is not indoors.\"\\n\\nWe need to check if the terms \"mother\" and \"woman\" can be assumed to be equivalent in this context. Since all mothers are women, we can safely make this assumption.\\n\\nNext, we need to ensure that the rest of the context aligns with the question. The context mentions that the mother was woken up and that she was not indoors, which matches the situation described in the question.'},\n",
              "  'id': UUID('43e5cb35-e7cf-4fff-b441-fd7917d6cc9c')},\n",
              " {'input': {'context': 'There is not a single person walking in the city.',\n",
              "   'question': 'Can we logically conclude for sure that there is not a single musher walking in the city?'},\n",
              "  'output': {'is_entailed': 'Yes',\n",
              "   'reasoning': \"Reasoning: Let's think step by step in order to produce the answer. We know from the context that there is not a single person walking in the city. A musher is a type of person, specifically someone who drives a dog sled. Since the context states there are no people walking in the city, this would include all types of people, including mushers.\"},\n",
              "  'id': UUID('2e61737e-452a-46b1-abdb-eff4a5a1c4d5')},\n",
              " {'input': {'context': 'The girl who is not weak is wearing some jewelry, but not a dress.',\n",
              "   'question': 'Can we logically conclude for sure that the girl who is not weak is wearing some rings, but not a dress?'},\n",
              "  'output': {'is_entailed': 'No',\n",
              "   'reasoning': \"Reasoning: Let's think step by step in order to produce the answer. We know from the context that the girl who is not weak is wearing some jewelry but not a dress. However, the context does not specify what type of jewelry she is wearing. Jewelry could include rings, necklaces, bracelets, earrings, etc. Therefore, we cannot logically conclude for sure that the girl is wearing rings specifically.\"},\n",
              "  'id': UUID('bde6abce-9288-4a3b-9a4e-4a7642e9cdc7')},\n",
              " {'input': {'context': 'The girl who is not here is not wearing any jewelry at all.',\n",
              "   'question': 'Can we logically conclude for sure that the girl who is not here is not wearing any ringlets at all?'},\n",
              "  'output': {'is_entailed': 'Yes',\n",
              "   'reasoning': 'Reasoning: Let\\'s think step by step in order to produce the answer. We start by understanding the context: \"The girl who is not here is not wearing any jewelry at all.\" This statement means that any form of jewelry, including ringlets, is not being worn by the girl who is not present. Since ringlets are a type of jewelry, if she is not wearing any jewelry, she is also not wearing ringlets.'},\n",
              "  'id': UUID('d2050091-b628-40bb-a3de-22060bdba537')},\n",
              " {'input': {'context': 'The person who is not allergic likes raspberries but prefers not to eat vegetables.',\n",
              "   'question': 'Can we logically conclude for sure that the person who is not allergic likes fruit but prefers not to eat vegetables?'},\n",
              "  'output': {'is_entailed': 'Yes',\n",
              "   'reasoning': \"Reasoning: Let's think step by step in order to produce the answer. We know from the context that the person who is not allergic likes raspberries and prefers not to eat vegetables. Raspberries are a type of fruit. Therefore, it is logical to conclude that the person likes at least one type of fruit (raspberries). Since the context also states that the person prefers not to eat vegetables, it aligns with the question's statement about their preference regarding vegetables.\"},\n",
              "  'id': UUID('82109203-1216-494a-9ed4-8b396d2f3c56')},\n",
              " {'input': {'context': 'A man is holding something in his hands.',\n",
              "   'question': 'Can we logically conclude for sure that a man is holding stamps in his hands?'},\n",
              "  'output': {'is_entailed': 'No',\n",
              "   'reasoning': 'Reasoning: Let\\'s think step by step in order to produce the answer. We know from the context that a man is holding something in his hands. However, the context does not specify what that \"something\" is. It could be anything, not necessarily stamps. Therefore, we cannot logically conclude for sure that the man is holding stamps in his hands.'},\n",
              "  'id': UUID('9f11d197-e72a-4307-b39f-2d5e89f5dc5e')}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(best_examples)"
      ],
      "metadata": {
        "id": "X7AitviDkzfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = create_chain()\n",
        "# This time we will apply gpt-3.5-turbo, but use the few-shot examples + reasoning trajectories\n",
        "# from gpt-4o to help induce better performance\n",
        "best_performing_model = create_chain(best_examples)"
      ],
      "metadata": {
        "id": "4BV687uNgEtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_test_name"
      ],
      "metadata": {
        "id": "cO6Jwi9Te2ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, model in [\n",
        "    (\"optimized\", best_performing_model),\n",
        "    # (\"original\", original_model),\n",
        "]:\n",
        "    client.run_on_dataset(\n",
        "        dataset_name=full_test_name,\n",
        "        llm_or_chain_factory=model,\n",
        "        evaluation=eval_config,\n",
        "        verbose=True,\n",
        "        project_metadata={\n",
        "            \"model\": model_name,\n",
        "        },\n",
        "    )"
      ],
      "metadata": {
        "id": "kNfOZbz9gEqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot examples을 추가하여 사용하여 **성능을 약 0.54에서 약 0.95로 향상**시킬 수 있었습니다!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N4efURdgZPHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 성능 비교를 위해 Few-shot examples를 적용하지 않은 상태에서 성능을 측정해보기"
      ],
      "metadata": {
        "id": "G9SkjA3pnbFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = create_chain()\n",
        "# This time we will apply gpt-3.5-turbo, but use the few-shot examples + reasoning trajectories\n",
        "# from gpt-4o to help induce better performance\n",
        "best_performing_model = create_chain(best_examples)"
      ],
      "metadata": {
        "id": "Yw5WNvZ-nag8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, model in [\n",
        "    # (\"optimized\", best_performing_model),\n",
        "    (\"original\", original_model),\n",
        "]:\n",
        "    client.run_on_dataset(\n",
        "        dataset_name=full_test_name,\n",
        "        llm_or_chain_factory=model,\n",
        "        evaluation=eval_config,\n",
        "        verbose=True,\n",
        "        project_metadata={\n",
        "            \"model\": model_name,\n",
        "        },\n",
        "    )"
      ],
      "metadata": {
        "id": "VTnBCkqhnag9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7U-Z7j6KgEjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}